# EXP-001: The Illusion of Success in Naive Evaluation

**Status:** Active
**Date:** 2024-12-27
**Type:** Foundation / Process

## 1. The Problem
Machine learning workflows often produce models that appear effective during development ("high accuracy") but fail to generalize or perform safely in real-world contexts. [cite_start]We suspect this discrepancy stems from fundamental flaws in how data is split, preprocessed, and evaluated, specifically through "naive" baselines that mask failure modes[cite: 104, 153].

## 2. Why It Matters
Evaluation is the bedrock of valid research. If our measurement tools (metrics, splitting strategies) are flawed, every downstream experiment in NDX Intelligence will be unreliable. [cite_start]We must prove *why* standard practices fail to establish a rigorous "ground truth" for future work[cite: 105].

## 3. Initial Assumptions & Hypotheses
We assume the following "standard" practices will generate misleading signals:
* [cite_start]**Assumption A (Leakage):** Standardizing data (mean/variance) *before* splitting leaks information from the test set into the training set, artificially inflating performance[cite: 119].
* [cite_start]**Assumption B (Metric Deception):** On a 95:5 imbalanced dataset, optimizing for "Accuracy" will yield a high score (~95%) even if the model fails to identify a single minority class instance[cite: 121].
* [cite_start]**Assumption C (The "Works on My Machine" Trap):** A model can be mathematically "correct" but practically useless if the evaluation metric does not align with the business/research objective[cite: 126].

## 4. Experiment Setup (The Naive Baseline)
To test these assumptions, we will intentionally implement a flawed workflow:
* **Data:** Synthetic Binary Classification (10,000 samples, 20 features, 95% Class 0 / 5% Class 1).
* **Model:** Logistic Regression (scikit-learn, default parameters).
* **Flawed Process:**
    1.  **Global Scaling:** Apply `StandardScaler` to the *entire* dataset first.
    2.  **Random Split:** Use standard `train_test_split` (80/20).
    3.  **Vanity Metric:** Evaluate using `accuracy_score` only.

## 5. Expected Failures (What We Want to Break)
* We expect the model to report >95% accuracy.
* We expect the Confusion Matrix to reveal near-zero recall for Class 1 (the signal we actually care about).
* [cite_start]We expect this "high performance" to be a mirage caused by the imbalance and leakage[cite: 108, 126].

## 6. Links & Resources
* **Workspace:** `/experiments/EXP-001/`
* **Notebooks:** `/experiments/EXP-001/notebooks/`
* **Notes:**
    * [Assumptions](/notes/assumptions.md)
    * [Failures](/notes/failures.md)
