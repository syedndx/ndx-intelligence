# EXP-001: The Illusion of Success in Naive Evaluation

**Status:** Active  
**Date:** 2024-12-27  
**Type:** Foundation / Process

## 1. The Problem
Machine learning workflows often produce models that appear effective during development ("high accuracy") but fail to generalize or perform safely in real-world contexts. We suspect this discrepancy stems from fundamental flaws in how data is split, preprocessed, and evaluated, specifically through naive baselines that mask failure modes.

## 2. Why It Matters
Evaluation is the bedrock of valid research. If our measurement tools (metrics, splitting strategies) are flawed, every downstream experiment in NDX Intelligence becomes unreliable. This experiment exists to explain *why* standard practices fail and to establish a rigorous baseline for future work.

## 3. Initial Assumptions & Hypotheses
We assume the following standard practices will generate misleading signals:

* **Assumption A (Leakage):**  
  Standardizing data (mean/variance) *before* splitting leaks information from the test set into the training set, artificially inflating performance.

* **Assumption B (Metric Deception):**  
  On a 95:5 imbalanced dataset, optimizing for accuracy will yield a high score (~95%) even if the model fails to identify a single minority class instance.

* **Assumption C (The “Works on My Machine” Trap):**  
  A model can be mathematically correct but practically useless if the evaluation metric does not align with the real objective of the task.

## 4. Experiment Setup (The Naive Baseline)
To test these assumptions, we will intentionally implement a flawed workflow:

* **Data:** Synthetic binary classification  
  (10,000 samples, 20 features, 95% Class 0 / 5% Class 1)

* **Model:** Logistic Regression (scikit-learn, default parameters)

* **Flawed Process:**
  1. **Global Scaling:** Apply `StandardScaler` to the entire dataset before splitting.
  2. **Random Split:** Use a standard `train_test_split` (80/20).
  3. **Vanity Metric:** Evaluate using `accuracy_score` only.

## 5. Expected Failures (What We Want to Break)
* The model will report >95% accuracy.
* The confusion matrix will reveal near-zero recall for Class 1.
* The apparent high performance will be a mirage caused by class imbalance and leakage rather than genuine learning.

## 6. Links & Resources
* **Workspace:** `/experiments/EXP-001/`
* **Notebooks:** `/experiments/EXP-001/notebooks/`
* **Notes:**
  * `/notes/assumptions.md`
  * `/notes/failures.md`
